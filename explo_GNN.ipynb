{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "########## INSTALL TORCH GEOMETRIC ##################\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "#####################################################\n",
        "import torch\n",
        "\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split(\"+\")[0]\n",
        "\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return \"cu\" + version.replace(\".\", \"\")\n",
        "\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AwBZz7LuWPp",
        "outputId": "e801bf52-6c00-4c24-e842-4072ba02e009"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (932 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.1/932.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7jCqk-F5Kllc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATv2Conv, SAGEConv\n",
        "from torch_geometric.loader import NeighborLoader, LinkNeighborLoader, LinkLoader, NodeLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_information = pd.read_csv('node_information.csv', header=None, index_col=0)\n",
        "train_set = pd.read_csv('train.txt', header=None, sep=' ')\n",
        "train_set.columns = ['node1', 'node2', 'edge']\n",
        "test_set = pd.read_csv('test.txt', header=None, sep=' ')\n",
        "test_set.columns = ['node1', 'node2']\n",
        "\n",
        "mapping = {node_information.index[i]: i for i in range(len(node_information))}\n",
        "node_information.index = node_information.index.map(mapping)\n",
        "train_set[\"node1\"] = train_set[\"node1\"].map(mapping)\n",
        "train_set[\"node2\"] = train_set[\"node2\"].map(mapping)\n",
        "test_set[\"node1\"] = test_set[\"node1\"].map(mapping)\n",
        "test_set[\"node2\"] = test_set[\"node2\"].map(mapping)"
      ],
      "metadata": {
        "id": "UAMFQlkdNVQf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[\"edge\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3zJ82DEudbS",
        "outputId": "0d8dd2d0-27f2-48a8-b17b-d267055b96bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5248\n",
              "0    5248\n",
              "Name: edge, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(N, train_ratio, seed=4):\n",
        "    \"\"\" Creates train/val/test masks\n",
        "\n",
        "    Args:\n",
        "        N (int): dataset size\n",
        "        train_ratio (float): proportion of the training set\n",
        "        seed (int, optional): Fixes random. Defaults to 4\n",
        "\n",
        "    Return:\n",
        "        [tensors]: returns boolean tensors for train/val/test set\n",
        "        True indicates that a node belong to this set, False otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    train_size = int(train_ratio * N)\n",
        "    val_size = N - train_size\n",
        "\n",
        "    # split dataset\n",
        "    subsets = torch.utils.data.random_split(range(N), lengths = [train_size, val_size, 0], generator=torch.Generator().manual_seed(seed))\n",
        "    train_inds, val_inds, test_inds = [torch.Tensor(subset.indices) for subset in subsets]\n",
        "\n",
        "    # create tensors of masks for each subset\n",
        "    dataset_inds = torch.arange(N)\n",
        "    train_mask = torch.isin(dataset_inds, train_inds)\n",
        "    val_mask = torch.isin(dataset_inds, val_inds)\n",
        "    test_mask = torch.isin(dataset_inds, test_inds)\n",
        "\n",
        "    return train_mask, val_mask\n",
        "\n",
        "train_mask, val_mask = split_dataset(train_set.shape[0], train_ratio=0.8)"
      ],
      "metadata": {
        "id": "yzwCY6Meud3Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "degree_per_nodes = np.array([train_set[pd.Series(train_mask)].loc[(train_set[\"node1\"] == i) | (train_set[\"node2\"] == i), \"edge\"].sum() for i in range(len(node_information))])\n",
        "degree_per_nodes_standardized = StandardScaler().fit_transform(degree_per_nodes.reshape(-1,1))\n",
        "\n",
        "node_information[933] = degree_per_nodes_standardized"
      ],
      "metadata": {
        "id": "SisSz5tayWOU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, hidden_size_bis, output_size_embed):\n",
        "    super().__init__()\n",
        "    # self.conv1 = GATv2Conv(input_size, hidden_size, heads=6)\n",
        "    # self.conv2 = GATv2Conv(6 * hidden_size, hidden_size, heads=4)\n",
        "    # self.conv3 = GATv2Conv(4 * hidden_size, hidden_size_bis, heads=4)\n",
        "    # self.conv4 = GATv2Conv(4 * hidden_size_bis, output_size_embed, heads=6, concat=False)\n",
        "\n",
        "    self.conv1 = SAGEConv(input_size, hidden_size, aggr=\"max\")\n",
        "    self.conv2 = SAGEConv(hidden_size, hidden_size_bis, aggr=\"mean\")\n",
        "    self.conv3 = SAGEConv(hidden_size_bis, hidden_size_bis//2, aggr=\"max\")\n",
        "    self.conv4 = SAGEConv(hidden_size_bis//2, hidden_size_bis//2, aggr=\"mean\")\n",
        "    self.conv5 = SAGEConv(hidden_size_bis//2, output_size_embed, aggr=\"max\")\n",
        "\n",
        "    self.lin1 = nn.Linear(2 * output_size_embed, output_size_embed)\n",
        "    self.lin2 = nn.Linear(output_size_embed, output_size_embed)\n",
        "    self.lin3 = nn.Linear(output_size_embed, output_size_embed//2)\n",
        "    self.lin4 = nn.Linear(output_size_embed//2, 2)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.conv3(x, edge_index)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.conv4(x, edge_index)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.conv5(x, edge_index)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "\n",
        "    row, col = edge_index\n",
        "    x = torch.cat([x[row], x[col]], dim=1)\n",
        "    x = self.lin1(x)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.lin2(x)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.lin3(x)\n",
        "    x = nn.functional.elu(x)\n",
        "    x = nn.functional.dropout(x, p=0.5)\n",
        "    x = self.lin4(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "XCUbGbO-ufdV"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_fcn, device, dataloader):\n",
        "    score_list_batch = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch.x, batch.edge_index)\n",
        "        loss_test = loss_fcn(output, batch.y.float())\n",
        "        predict = np.where(output.detach().cpu().numpy()[:, 1] >= 0, 1, 0)\n",
        "        score = accuracy_score(batch.y.cpu().numpy()[:, 1], predict)\n",
        "        score_list_batch.append(score)\n",
        "\n",
        "    return np.array(score_list_batch).mean()"
      ],
      "metadata": {
        "id": "OnIzeM9Cuhi0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            train_batch_device = train_batch.to(device)\n",
        "            # logits is the output of the model\n",
        "            output = model(train_batch_device.x, train_batch_device.edge_index)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(output, train_batch_device.y.float())\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"Accuracy Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ],
      "metadata": {
        "id": "2ZLlJBtBujFy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(node_information.values, dtype=torch.float)\n",
        "\n",
        "y_train = torch.tensor(train_set['edge'].values[train_mask], dtype=torch.long)\n",
        "y_train = torch.tensor([[1 if y_train[i]==0 else 0, 1 if y_train[i]==1 else 0] for i in range(len(y_train))])\n",
        "edge_index_train = torch.tensor(train_set[['node1', 'node2']].values.T[torch.vstack([train_mask, train_mask])].reshape(2, -1), dtype=torch.long)\n",
        "\n",
        "y_val = torch.tensor(train_set['edge'].values[val_mask], dtype=torch.long)\n",
        "y_val = torch.tensor([[1 if y_val[i]==0 else 0, 1 if y_val[i]==1 else 0] for i in range(len(y_val))])\n",
        "edge_index_val = torch.tensor(train_set[['node1', 'node2']].values.T[torch.vstack([val_mask, val_mask])].reshape(2, -1), dtype=torch.long)\n",
        "\n",
        "edge_index_test = torch.tensor(test_set[['node1', 'node2']].values.T, dtype=torch.long)\n",
        "\n",
        "data_train = Data(x=x, edge_index=edge_index_train, y=y_train)\n",
        "data_val = Data(x=x, edge_index=edge_index_val, y=y_val)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_data_loader = NeighborLoader(data_train, num_neighbors=[-1], batch_size=batch_size, shuffle=True)\n",
        "val_data_loader = NeighborLoader(data_val, num_neighbors=[-1], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "LUH31khFuk60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab8b9bb-9118-46d4-e1c1-75a228352c9c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "# Create the GAT model\n",
        "model = GATModel(input_size=x.shape[1], hidden_size=500, hidden_size_bis=400, output_size_embed=300).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.005)\n",
        "\n",
        "max_epochs=200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkl1NrslumSg",
        "outputId": "1abb3708-8f00-40ae-b9f9-e67327a626bf"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with mini-batches\n",
        "epoch_list, model_scores = train(\n",
        "    model,\n",
        "    loss_fcn,\n",
        "    device,\n",
        "    optimizer,\n",
        "    max_epochs,\n",
        "    train_data_loader,\n",
        "    val_data_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "T5sabRrevhwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on val set batch\n",
        "\n",
        "score_list_batch = []\n",
        "predict_list_batch = []\n",
        "y_list_batch = []\n",
        "\n",
        "model.eval()\n",
        "for i, batch in enumerate(val_data_loader):\n",
        "    batch = batch.to(device)\n",
        "    output = model(batch.x, batch.edge_index)\n",
        "    predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
        "    score = accuracy_score(batch.y.cpu().numpy()[:, 1], predict[:, 1])\n",
        "\n",
        "    score_list_batch.append(score)\n",
        "    predict_list_batch.append(predict)\n",
        "    y_list_batch.append(batch.y.cpu().numpy()[:, 1].mean())"
      ],
      "metadata": {
        "id": "lq_81TzIVxr_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array((score_list_batch)).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDJlrw_G1Tgo",
        "outputId": "df24b3ed-fd66-4367-8f47-8b3c7477765e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.635291542545112"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train without mini-batches\n",
        "data_train_device = data_train.to(device)\n",
        "data_val_device = data_val.to(device)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data_train_device.x, data_train_device.edge_index)\n",
        "    loss = loss_fcn(out, data_train_device.y.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss.item()))\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    output = model(data_val_device.x, data_val_device.edge_index)\n",
        "    loss_test = loss_fcn(output, data_val_device.y.float())\n",
        "    predict = np.where(output.detach().cpu().numpy()[:, 1] >= 0.5, 1, 0)\n",
        "    score = accuracy_score(data_val_device.y.cpu().numpy()[:, 1], predict)\n",
        "    print(\"Accuracy core: {:.4f}\".format(score))"
      ],
      "metadata": {
        "id": "ql_nNxVq5D3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cb7dc2-04cb-4c4a-c65a-95fe7426a239"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00001 | Loss: 0.4077\n",
            "Accuracy core: 0.6567\n",
            "Epoch 00002 | Loss: 0.4155\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00003 | Loss: 0.4213\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00004 | Loss: 0.4268\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00005 | Loss: 0.4148\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00006 | Loss: 0.4242\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00007 | Loss: 0.4163\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00008 | Loss: 0.4213\n",
            "Accuracy core: 0.6705\n",
            "Epoch 00009 | Loss: 0.4162\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00010 | Loss: 0.4096\n",
            "Accuracy core: 0.6567\n",
            "Epoch 00011 | Loss: 0.4176\n",
            "Accuracy core: 0.6562\n",
            "Epoch 00012 | Loss: 0.4082\n",
            "Accuracy core: 0.6600\n",
            "Epoch 00013 | Loss: 0.4162\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00014 | Loss: 0.4126\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00015 | Loss: 0.4112\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00016 | Loss: 0.4126\n",
            "Accuracy core: 0.6710\n",
            "Epoch 00017 | Loss: 0.4081\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00018 | Loss: 0.4154\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00019 | Loss: 0.4054\n",
            "Accuracy core: 0.6671\n",
            "Epoch 00020 | Loss: 0.4123\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00021 | Loss: 0.4116\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00022 | Loss: 0.4053\n",
            "Accuracy core: 0.6681\n",
            "Epoch 00023 | Loss: 0.4053\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00024 | Loss: 0.4060\n",
            "Accuracy core: 0.6576\n",
            "Epoch 00025 | Loss: 0.4073\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00026 | Loss: 0.4080\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00027 | Loss: 0.4093\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00028 | Loss: 0.4037\n",
            "Accuracy core: 0.6562\n",
            "Epoch 00029 | Loss: 0.4074\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00030 | Loss: 0.4051\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00031 | Loss: 0.4016\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00032 | Loss: 0.4028\n",
            "Accuracy core: 0.6729\n",
            "Epoch 00033 | Loss: 0.4076\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00034 | Loss: 0.4065\n",
            "Accuracy core: 0.6686\n",
            "Epoch 00035 | Loss: 0.4057\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00036 | Loss: 0.4070\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00037 | Loss: 0.4049\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00038 | Loss: 0.4042\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00039 | Loss: 0.4076\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00040 | Loss: 0.4095\n",
            "Accuracy core: 0.6571\n",
            "Epoch 00041 | Loss: 0.4043\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00042 | Loss: 0.4054\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00043 | Loss: 0.4093\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00044 | Loss: 0.4069\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00045 | Loss: 0.4028\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00046 | Loss: 0.4117\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00047 | Loss: 0.4162\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00048 | Loss: 0.4152\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00049 | Loss: 0.4079\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00050 | Loss: 0.4094\n",
            "Accuracy core: 0.6552\n",
            "Epoch 00051 | Loss: 0.4103\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00052 | Loss: 0.4064\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00053 | Loss: 0.4074\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00054 | Loss: 0.4101\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00055 | Loss: 0.4063\n",
            "Accuracy core: 0.6714\n",
            "Epoch 00056 | Loss: 0.4067\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00057 | Loss: 0.4088\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00058 | Loss: 0.4079\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00059 | Loss: 0.4065\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00060 | Loss: 0.4077\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00061 | Loss: 0.4128\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00062 | Loss: 0.4026\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00063 | Loss: 0.4097\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00064 | Loss: 0.4093\n",
            "Accuracy core: 0.6671\n",
            "Epoch 00065 | Loss: 0.4064\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00066 | Loss: 0.4105\n",
            "Accuracy core: 0.6700\n",
            "Epoch 00067 | Loss: 0.4130\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00068 | Loss: 0.4132\n",
            "Accuracy core: 0.6705\n",
            "Epoch 00069 | Loss: 0.4059\n",
            "Accuracy core: 0.6686\n",
            "Epoch 00070 | Loss: 0.4110\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00071 | Loss: 0.4064\n",
            "Accuracy core: 0.6648\n",
            "Epoch 00072 | Loss: 0.4039\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00073 | Loss: 0.4058\n",
            "Accuracy core: 0.6567\n",
            "Epoch 00074 | Loss: 0.4084\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00075 | Loss: 0.4057\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00076 | Loss: 0.4040\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00077 | Loss: 0.4048\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00078 | Loss: 0.4071\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00079 | Loss: 0.4052\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00080 | Loss: 0.4062\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00081 | Loss: 0.4029\n",
            "Accuracy core: 0.6648\n",
            "Epoch 00082 | Loss: 0.4058\n",
            "Accuracy core: 0.6557\n",
            "Epoch 00083 | Loss: 0.4030\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00084 | Loss: 0.4054\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00085 | Loss: 0.4092\n",
            "Accuracy core: 0.6576\n",
            "Epoch 00086 | Loss: 0.4079\n",
            "Accuracy core: 0.6590\n",
            "Epoch 00087 | Loss: 0.4050\n",
            "Accuracy core: 0.6552\n",
            "Epoch 00088 | Loss: 0.4069\n",
            "Accuracy core: 0.6590\n",
            "Epoch 00089 | Loss: 0.4024\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00090 | Loss: 0.4033\n",
            "Accuracy core: 0.6538\n",
            "Epoch 00091 | Loss: 0.4043\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00092 | Loss: 0.4081\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00093 | Loss: 0.4131\n",
            "Accuracy core: 0.6600\n",
            "Epoch 00094 | Loss: 0.4089\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00095 | Loss: 0.4075\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00096 | Loss: 0.4069\n",
            "Accuracy core: 0.6576\n",
            "Epoch 00097 | Loss: 0.4083\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00098 | Loss: 0.4045\n",
            "Accuracy core: 0.6600\n",
            "Epoch 00099 | Loss: 0.4035\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00100 | Loss: 0.4079\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00101 | Loss: 0.4052\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00102 | Loss: 0.4058\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00103 | Loss: 0.4078\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00104 | Loss: 0.4123\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00105 | Loss: 0.4094\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00106 | Loss: 0.4056\n",
            "Accuracy core: 0.6676\n",
            "Epoch 00107 | Loss: 0.4101\n",
            "Accuracy core: 0.6690\n",
            "Epoch 00108 | Loss: 0.4059\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00109 | Loss: 0.4085\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00110 | Loss: 0.4129\n",
            "Accuracy core: 0.6624\n",
            "Epoch 00111 | Loss: 0.4078\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00112 | Loss: 0.4045\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00113 | Loss: 0.4109\n",
            "Accuracy core: 0.6676\n",
            "Epoch 00114 | Loss: 0.4027\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00115 | Loss: 0.4024\n",
            "Accuracy core: 0.6690\n",
            "Epoch 00116 | Loss: 0.4068\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00117 | Loss: 0.4085\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00118 | Loss: 0.4050\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00119 | Loss: 0.4091\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00120 | Loss: 0.4065\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00121 | Loss: 0.4050\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00122 | Loss: 0.4081\n",
            "Accuracy core: 0.6548\n",
            "Epoch 00123 | Loss: 0.4093\n",
            "Accuracy core: 0.6595\n",
            "Epoch 00124 | Loss: 0.4049\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00125 | Loss: 0.4031\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00126 | Loss: 0.4055\n",
            "Accuracy core: 0.6714\n",
            "Epoch 00127 | Loss: 0.4047\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00128 | Loss: 0.4045\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00129 | Loss: 0.4065\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00130 | Loss: 0.4075\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00131 | Loss: 0.3999\n",
            "Accuracy core: 0.6648\n",
            "Epoch 00132 | Loss: 0.4084\n",
            "Accuracy core: 0.6590\n",
            "Epoch 00133 | Loss: 0.4056\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00134 | Loss: 0.4144\n",
            "Accuracy core: 0.6581\n",
            "Epoch 00135 | Loss: 0.4190\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00136 | Loss: 0.4105\n",
            "Accuracy core: 0.6686\n",
            "Epoch 00137 | Loss: 0.4186\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00138 | Loss: 0.4112\n",
            "Accuracy core: 0.6676\n",
            "Epoch 00139 | Loss: 0.4051\n",
            "Accuracy core: 0.6648\n",
            "Epoch 00140 | Loss: 0.4116\n",
            "Accuracy core: 0.6648\n",
            "Epoch 00141 | Loss: 0.4191\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00142 | Loss: 0.4088\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00143 | Loss: 0.4095\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00144 | Loss: 0.4116\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00145 | Loss: 0.4063\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00146 | Loss: 0.4118\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00147 | Loss: 0.4059\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00148 | Loss: 0.4024\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00149 | Loss: 0.4075\n",
            "Accuracy core: 0.6681\n",
            "Epoch 00150 | Loss: 0.4052\n",
            "Accuracy core: 0.6571\n",
            "Epoch 00151 | Loss: 0.4053\n",
            "Accuracy core: 0.6600\n",
            "Epoch 00152 | Loss: 0.4045\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00153 | Loss: 0.4017\n",
            "Accuracy core: 0.6543\n",
            "Epoch 00154 | Loss: 0.4024\n",
            "Accuracy core: 0.6586\n",
            "Epoch 00155 | Loss: 0.4051\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00156 | Loss: 0.4023\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00157 | Loss: 0.4019\n",
            "Accuracy core: 0.6695\n",
            "Epoch 00158 | Loss: 0.4035\n",
            "Accuracy core: 0.6505\n",
            "Epoch 00159 | Loss: 0.4054\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00160 | Loss: 0.4036\n",
            "Accuracy core: 0.6638\n",
            "Epoch 00161 | Loss: 0.4014\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00162 | Loss: 0.4041\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00163 | Loss: 0.4021\n",
            "Accuracy core: 0.6681\n",
            "Epoch 00164 | Loss: 0.4043\n",
            "Accuracy core: 0.6576\n",
            "Epoch 00165 | Loss: 0.4044\n",
            "Accuracy core: 0.6676\n",
            "Epoch 00166 | Loss: 0.4010\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00167 | Loss: 0.4068\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00168 | Loss: 0.4018\n",
            "Accuracy core: 0.6624\n",
            "Epoch 00169 | Loss: 0.4039\n",
            "Accuracy core: 0.6633\n",
            "Epoch 00170 | Loss: 0.4062\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00171 | Loss: 0.4022\n",
            "Accuracy core: 0.6686\n",
            "Epoch 00172 | Loss: 0.4053\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00173 | Loss: 0.4118\n",
            "Accuracy core: 0.6695\n",
            "Epoch 00174 | Loss: 0.4071\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00175 | Loss: 0.4048\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00176 | Loss: 0.4119\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00177 | Loss: 0.4159\n",
            "Accuracy core: 0.6643\n",
            "Epoch 00178 | Loss: 0.4068\n",
            "Accuracy core: 0.6581\n",
            "Epoch 00179 | Loss: 0.4037\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00180 | Loss: 0.4076\n",
            "Accuracy core: 0.6614\n",
            "Epoch 00181 | Loss: 0.4057\n",
            "Accuracy core: 0.6671\n",
            "Epoch 00182 | Loss: 0.4068\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00183 | Loss: 0.4145\n",
            "Accuracy core: 0.6590\n",
            "Epoch 00184 | Loss: 0.4135\n",
            "Accuracy core: 0.6657\n",
            "Epoch 00185 | Loss: 0.4065\n",
            "Accuracy core: 0.6567\n",
            "Epoch 00186 | Loss: 0.4077\n",
            "Accuracy core: 0.6610\n",
            "Epoch 00187 | Loss: 0.4046\n",
            "Accuracy core: 0.6667\n",
            "Epoch 00188 | Loss: 0.4095\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00189 | Loss: 0.4056\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00190 | Loss: 0.4028\n",
            "Accuracy core: 0.6605\n",
            "Epoch 00191 | Loss: 0.4053\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00192 | Loss: 0.4048\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00193 | Loss: 0.4066\n",
            "Accuracy core: 0.6629\n",
            "Epoch 00194 | Loss: 0.4054\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00195 | Loss: 0.4024\n",
            "Accuracy core: 0.6652\n",
            "Epoch 00196 | Loss: 0.4013\n",
            "Accuracy core: 0.6619\n",
            "Epoch 00197 | Loss: 0.4058\n",
            "Accuracy core: 0.6600\n",
            "Epoch 00198 | Loss: 0.4035\n",
            "Accuracy core: 0.6590\n",
            "Epoch 00199 | Loss: 0.4022\n",
            "Accuracy core: 0.6662\n",
            "Epoch 00200 | Loss: 0.4041\n",
            "Accuracy core: 0.6629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = model(x.to(device), edge_index_test.to(device))\n",
        "pred_test = np.where(output.detach().cpu().numpy() >= 0, 1, 0)[:, 1]"
      ],
      "metadata": {
        "id": "gDWZ415PMYT4"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyb1xNC_MoN9",
        "outputId": "48ba2aac-1f12-40f2-bc6c-2360e74a8e48"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = zip(np.array(range(len(test_set))), pred_test)"
      ],
      "metadata": {
        "id": "X9udbYt0_G7e"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "i = len(os.listdir(\"models\")) +1\n",
        "model_path = f\"models/model_{i}.csv\"\n",
        "\n",
        "with open(model_path,\"w\") as pred:\n",
        "    csv_out = csv.writer(pred)\n",
        "    csv_out.writerow(i for i in [\"ID\", \"Predicted\"])\n",
        "    for row in preds:\n",
        "         csv_out.writerow(row)\n",
        "    pred.close()\n"
      ],
      "metadata": {
        "id": "pQGhCc2q_5wx"
      },
      "execution_count": 199,
      "outputs": []
    }
  ]
}
